{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwa0q8OvzcHajZ5zOG7dSV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!git clone https://github.com/karpathy/minGPT.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BdsyIqtX8ZjZ","executionInfo":{"status":"ok","timestamp":1681186838604,"user_tz":-480,"elapsed":787,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}},"outputId":"32d02e20-231d-4821-abcf-e9b23bf4de80"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'minGPT'...\n","remote: Enumerating objects: 489, done.\u001b[K\n","remote: Total 489 (delta 0), reused 0 (delta 0), pack-reused 489\u001b[K\n","Receiving objects: 100% (489/489), 1.44 MiB | 23.38 MiB/s, done.\n","Resolving deltas: 100% (260/260), done.\n"]}]},{"cell_type":"code","source":["cd minGPT"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gFkW3Bxr8d1U","executionInfo":{"status":"ok","timestamp":1681186838605,"user_tz":-480,"elapsed":7,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}},"outputId":"6da5bf46-24cc-4757-ca7a-6437c8ec9e7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/minGPT\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xMoA45LY7IpS"},"outputs":[],"source":["\"\"\"\n","Trains a GPT to add n-digit numbers.\n","\"\"\"\n","\n","import os\n","import sys\n","import json\n","\n","import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data.dataloader import DataLoader\n","\n","from mingpt.model import GPT\n","from mingpt.trainer import Trainer\n","from mingpt.utils import set_seed, setup_logging, CfgNode as CN"]},{"cell_type":"code","source":["import random\n","# -----------------------------------------------------------------------------\n","\n","def get_config():\n","\n","    C = CN()\n","\n","    # system\n","    C.system = CN()\n","    C.system.seed = 3407\n","    C.system.work_dir = './out/adder'\n","\n","    # data\n","    C.data = AdditionDataset.get_default_config()\n","\n","    # model\n","    C.model = GPT.get_default_config()\n","    C.model.model_type = 'gpt-nano'\n","\n","    # trainer\n","    C.trainer = Trainer.get_default_config()\n","    C.trainer.learning_rate = 5e-4 # the model we're using is so small that we can go a bit faster\n","\n","    return C\n","\n","# -----------------------------------------------------------------------------\n","\n","class AdditionDataset(Dataset):\n","    \"\"\"\n","    Creates n-digit addition problems. For example, if n=2, then an example\n","    addition problem would be to add 85 + 50 = 135. This problem would be\n","    represented as the following string for the GPT:\n","\n","    \"8550531\"\n","\n","    This is because:\n","    - we are discarding the + and =, which are not necessary. We just encode the digits\n","      of the input numbers concatenated together.\n","    - the result 135 is encoded backwards to make the addition easier to learn for the\n","      GPT model, because of how the addition algorithm works.\n","\n","    As one more example, the problem 6 + 39 = 45 would be encoded as:\n","\n","    \"0639054\"\n","\n","    where you will notice that we are padding with zeros to make sure that we always\n","    produce strings of the exact same size: n + n + (n + 1). When n=2, this is 7.\n","    At test time, we will feed in an addition problem by giving the first 2n digits,\n","    and hoping that the GPT model completes the sequence with the next (n+1) digits\n","    correctly.\n","    \"\"\"\n","\n","    @staticmethod\n","    def get_default_config():\n","        C = CN()\n","        C.ndigit = 10\n","        return C\n","\n","    def __init__(self, config, split):\n","        self.config = config\n","        self.split = split # train/test\n","\n","        # split up all addition problems into either training data or test data\n","        ndigit = self.config.ndigit\n","        # assert ndigit <= 3, \"the lines below would be very memory inefficient, in future maybe refactor to support\"\n","        num = 10000 # total number of possible addition problems with ndigit numbers\n","        rng = torch.Generator()\n","        rng.manual_seed(1337)\n","\n","        # perm = torch.randperm(num, generator=rng)\n","        # num_test = min(int(num*0.2), 500) # 20% of the whole dataset, or only up to 500\n","        # self.ixes = perm[:num_test] if split == 'test' else perm[num_test:]\n","        num_test = min(int(num * 0.2), 500)  # 20% of the whole dataset, or only up to 500\n","        self.test_start = 0\n","        self.test_end = num_test\n","        self.train_start = num_test\n","        self.train_end = num\n","\n","    def get_vocab_size(self):\n","        return 10 # digits 0..9\n","\n","    def get_block_size(self):\n","        # a,b,a+b, and +1 due to potential carry overflow,\n","        # but then also -1 because very last digit doesn't ever plug back\n","        # as there is no explicit <EOS> token to predict, it is implied\n","        return 3*self.config.ndigit + 1 - 1\n","\n","    def __len__(self):\n","        if self.split == 'test':\n","            return self.test_end - self.test_start\n","        else:\n","            return self.train_end - self.train_start\n","\n","    # def __getitem__(self, idx):\n","    #     ndigit = self.config.ndigit\n","    #     # given a problem index idx, first recover the associated a + b\n","    #     idx = self.ixes[idx].item()\n","    #     nd = 10**ndigit\n","        # a = idx // nd\n","        # b = idx %  nd\n","    #     # calculate the \"label\" of the addition problem a + b\n","    #     c = a + b\n","    #     # encode the digits of a, b, c into strings\n","    #     astr = f'%0{ndigit}d' % a\n","    #     bstr = f'%0{ndigit}d' % b\n","    #     cstr = (f'%0{ndigit+1}d' % c)[::-1] # reverse c to make addition easier\n","    #     render = astr + bstr + cstr\n","    #     dix = [int(s) for s in render] # convert each character to its token index\n","    #     # x will be input to GPT and y will be the associated expected outputs\n","    #     x = torch.tensor(dix[:-1], dtype=torch.long)\n","    #     y = torch.tensor(dix[1:], dtype=torch.long) # predict the next token in the sequence\n","    #     y[:ndigit*2-1] = -1 # we will only train in the output locations. -1 will mask loss to zero\n","    #     return x, y\n","\n","    # generate data on the fly\n","    def __getitem__(self, idx):\n","        ndigit = self.config.ndigit\n","        nd = 10**ndigit\n","\n","        if(idx < 2000):\n","          a = random.randint(0, 10)\n","          b = random.randint(0, 10)\n","        elif(idx < 3000):\n","          a = random.randint(0, 500)\n","          b = random.randint(0, 500)          \n","        elif(idx < 5000):\n","          a = random.randint(0, 1000)\n","          b = random.randint(0, 1000)          \n","        else:\n","          a = random.randint(0, 10**ndigit)\n","          b = random.randint(0, 10**ndigit)\n","\n","        # calculate the \"label\" of the addition problem a + b\n","        c = a + b\n","        # encode the digits of a, b, c into strings\n","        astr = f'%0{ndigit}d' % a\n","        bstr = f'%0{ndigit}d' % b\n","        cstr = (f'%0{ndigit+1}d' % c)[::-1]  # reverse c to make addition easier\n","        render = astr + bstr + cstr\n","        dix = [int(s) for s in render]  # convert each character to its token index\n","\n","        # x will be input to GPT and y will be the associated expected outputs\n","        x = torch.tensor(dix[:-1], dtype=torch.long)\n","        y = torch.tensor(dix[1:], dtype=torch.long)  # predict the next token in the sequence\n","        y[:ndigit * 2 - 1] = -1  # we will only train in the output locations. -1 will mask loss to zero\n","        return x, y\n"],"metadata":{"id":"GCMaSi968gN6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# get default config and overrides from the command line, if any\n","config = get_config()\n","# config.trainer.batch_size = 256\n","\n","# config.merge_from_args(sys.argv[1:])\n","print(config)\n","setup_logging(config)\n","set_seed(config.system.seed)\n","\n","# construct train and test datasets\n","train_dataset = AdditionDataset(config.data, split='train')\n","test_dataset  = AdditionDataset(config.data, split='test')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TubdxrL4EbWP","executionInfo":{"status":"ok","timestamp":1681186845415,"user_tz":-480,"elapsed":18,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}},"outputId":"4860bfb3-49ed-47f0-94c4-3cf78324fdcc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["system:\n","    seed: 3407\n","    work_dir: ./out/adder\n","data:\n","    ndigit: 10\n","model:\n","    model_type: gpt2\n","    n_layer: None\n","    n_head: None\n","    n_embd: None\n","    vocab_size: None\n","    block_size: None\n","    embd_pdrop: 0.1\n","    resid_pdrop: 0.1\n","    attn_pdrop: 0.1\n","trainer:\n","    device: auto\n","    num_workers: 4\n","    max_iters: None\n","    batch_size: 256\n","    learning_rate: 0.0005\n","    betas: (0.9, 0.95)\n","    weight_decay: 0.1\n","    grad_norm_clip: 1.0\n","\n"]}]},{"cell_type":"code","source":["len(train_dataset)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tyn1YJ0qFUzD","executionInfo":{"status":"ok","timestamp":1681186845415,"user_tz":-480,"elapsed":14,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}},"outputId":"34b64c12-b889-47ce-b7b5-9648a9faff7a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["9500"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# construct the model\n","config.model.vocab_size = train_dataset.get_vocab_size()\n","config.model.block_size = train_dataset.get_block_size()\n","model = GPT(config.model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LI8rMTGAEdHf","executionInfo":{"status":"ok","timestamp":1681186849541,"user_tz":-480,"elapsed":4136,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}},"outputId":"a31e4f0f-913f-4c6c-accc-6d759a06c457"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["number of parameters: 85.09M\n"]}]},{"cell_type":"code","source":["# construct the trainer object\n","trainer = Trainer(config.trainer, model, train_dataset)\n","\n","# helper function for the evaluation of a model\n","def eval_split(trainer, split, max_batches=None):\n","    dataset = {'train':train_dataset, 'test':test_dataset}[split]\n","    ndigit = config.data.ndigit\n","    results = []\n","    mistakes_printed_already = 0\n","    factors = torch.tensor([[10**i for i in range(ndigit+1)][::-1]]).to(trainer.device)\n","    loader = DataLoader(dataset, batch_size=100, num_workers=0, drop_last=False)\n","    for b, (x, y) in enumerate(loader):\n","        x = x.to(trainer.device)\n","        # isolate the first two digits of the input sequence alone\n","        d1d2 = x[:, :ndigit*2]\n","        # let the model sample the rest of the sequence\n","        d1d2d3 = model.generate(d1d2, ndigit+1, do_sample=False) # using greedy argmax, not sampling  \n","        # isolate the last digit of the sampled sequence\n","        d3 = d1d2d3[:, -(ndigit+1):]\n","        d3 = d3.flip(1) # reverse the digits to their \"normal\" order\n","        # decode the integers from individual digits\n","        d1i = (d1d2[:,:ndigit] * factors[:,1:]).sum(1)\n","        d2i = (d1d2[:,ndigit:ndigit*2] * factors[:,1:]).sum(1)\n","        d3i_pred = (d3 * factors).sum(1)\n","        d3i_gt = d1i + d2i # manually calculate the ground truth\n","        # evaluate the correctness of the results in this batch\n","        correct = (d3i_pred == d3i_gt).cpu() # Software 1.0 vs. Software 2.0 fight RIGHT on this line haha\n","        for i in range(x.size(0)):\n","            results.append(int(correct[i]))\n","            if not correct[i] and mistakes_printed_already < 5: # only print up to 5 mistakes to get a sense\n","                mistakes_printed_already += 1\n","                print(\"GPT claims that %d + %d = %d but gt is %d\" % (d1i[i], d2i[i], d3i_pred[i], d3i_gt[i]))\n","        if max_batches is not None and b+1 >= max_batches:\n","            break\n","    rt = torch.tensor(results, dtype=torch.float)\n","    print(\"%s final score: %d/%d = %.2f%% correct\" % (split, rt.sum(), len(results), 100*rt.mean()))\n","    return rt.sum()\n","\n","# iteration callback\n","top_score = 0\n","def batch_end_callback(trainer):\n","    global top_score\n","\n","    if trainer.iter_num % 10 == 0:\n","        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n","\n","    if trainer.iter_num % 500 == 0:\n","        # evaluate both the train and test score\n","        train_max_batches = {1: None, 2: None, 3: 5}[2] # if ndigit=2 we can afford the whole train set, ow no\n","        model.eval()\n","        with torch.no_grad():\n","            train_score = eval_split(trainer, 'train', max_batches=train_max_batches)\n","            test_score  = eval_split(trainer, 'test',  max_batches=None)\n","        score = train_score + test_score\n","        # save the model if this is the best score we've seen so far\n","        if score > top_score:\n","            top_score = score\n","            print(f\"saving model with new top score of {score}\")\n","            ckpt_path = os.path.join(config.system.work_dir, \"model.pt\")\n","            torch.save(model.state_dict(), ckpt_path)\n","        # revert model to training mode\n","        model.train()\n","\n","trainer.set_callback('on_batch_end', batch_end_callback)\n","\n","# run the optimization\n","trainer.run()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"gp6-PBRx8hXC","outputId":"28214492-9490-416c-8b85-636eeea90162","executionInfo":{"status":"error","timestamp":1681189481679,"user_tz":-480,"elapsed":2335790,"user":{"displayName":"Keith Hon","userId":"02304256421242879289"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["running on device cuda\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["iter_dt 0.00ms; iter 0: train loss 2.32139\n","GPT claims that 0 + 6 = 0 but gt is 6\n","GPT claims that 2 + 5 = 0 but gt is 7\n","GPT claims that 3 + 6 = 0 but gt is 9\n","GPT claims that 4 + 9 = 0 but gt is 13\n","GPT claims that 5 + 3 = 0 but gt is 8\n","train final score: 19/9500 = 0.20% correct\n","GPT claims that 1 + 8 = 0 but gt is 9\n","GPT claims that 8 + 2 = 0 but gt is 10\n","GPT claims that 3 + 8 = 0 but gt is 11\n","GPT claims that 3 + 3 = 0 but gt is 6\n","GPT claims that 8 + 2 = 0 but gt is 10\n","test final score: 4/500 = 0.80% correct\n","saving model with new top score of 23.0\n","iter_dt 1277.82ms; iter 10: train loss 2.03177\n","iter_dt 1279.19ms; iter 20: train loss 1.75505\n","iter_dt 1282.59ms; iter 30: train loss 1.46019\n","iter_dt 1284.49ms; iter 40: train loss 1.43966\n","iter_dt 1283.41ms; iter 50: train loss 1.38887\n","iter_dt 1281.54ms; iter 60: train loss 1.17438\n","iter_dt 1282.63ms; iter 70: train loss 1.26614\n","iter_dt 1288.04ms; iter 80: train loss 1.33078\n","iter_dt 1280.21ms; iter 90: train loss 1.24493\n","iter_dt 1279.11ms; iter 100: train loss 1.27264\n","iter_dt 1281.16ms; iter 110: train loss 1.22294\n","iter_dt 1281.69ms; iter 120: train loss 1.26327\n","iter_dt 1281.40ms; iter 130: train loss 1.29140\n","iter_dt 1281.03ms; iter 140: train loss 1.27633\n","iter_dt 1280.98ms; iter 150: train loss 1.27286\n","iter_dt 1280.76ms; iter 160: train loss 1.24518\n","iter_dt 1283.54ms; iter 170: train loss 1.23599\n","iter_dt 1282.93ms; iter 180: train loss 1.30119\n","iter_dt 1282.50ms; iter 190: train loss 1.19875\n","iter_dt 1285.46ms; iter 200: train loss 1.22605\n","iter_dt 1275.98ms; iter 210: train loss 1.15824\n","iter_dt 1281.91ms; iter 220: train loss 1.15228\n","iter_dt 1284.35ms; iter 230: train loss 1.15942\n","iter_dt 1285.39ms; iter 240: train loss 1.11464\n","iter_dt 1283.21ms; iter 250: train loss 1.14308\n","iter_dt 1284.44ms; iter 260: train loss 1.03031\n","iter_dt 1285.16ms; iter 270: train loss 1.06680\n","iter_dt 1284.68ms; iter 280: train loss 0.92458\n","iter_dt 1285.80ms; iter 290: train loss 0.97147\n","iter_dt 1287.37ms; iter 300: train loss 0.88768\n","iter_dt 1286.73ms; iter 310: train loss 0.94703\n","iter_dt 1280.30ms; iter 320: train loss 0.87660\n","iter_dt 1284.25ms; iter 330: train loss 0.90297\n","iter_dt 1284.90ms; iter 340: train loss 0.82035\n","iter_dt 1284.75ms; iter 350: train loss 0.79059\n","iter_dt 1285.57ms; iter 360: train loss 0.81201\n","iter_dt 1285.06ms; iter 370: train loss 0.83365\n","iter_dt 1284.35ms; iter 380: train loss 0.79067\n","iter_dt 1284.77ms; iter 390: train loss 0.75352\n","iter_dt 1285.71ms; iter 400: train loss 0.79533\n","iter_dt 1289.39ms; iter 410: train loss 0.68172\n","iter_dt 1285.20ms; iter 420: train loss 0.72876\n","iter_dt 1280.70ms; iter 430: train loss 0.82067\n","iter_dt 1282.98ms; iter 440: train loss 0.82129\n","iter_dt 1287.35ms; iter 450: train loss 0.67029\n","iter_dt 1284.37ms; iter 460: train loss 0.76761\n","iter_dt 1282.93ms; iter 470: train loss 0.71736\n","iter_dt 1281.23ms; iter 480: train loss 0.81525\n","iter_dt 1285.30ms; iter 490: train loss 0.71711\n","iter_dt 1284.77ms; iter 500: train loss 0.80813\n","GPT claims that 6 + 7 = 14 but gt is 13\n","GPT claims that 5 + 8 = 14 but gt is 13\n","GPT claims that 4 + 7 = 12 but gt is 11\n","GPT claims that 4 + 9 = 14 but gt is 13\n","GPT claims that 4 + 9 = 14 but gt is 13\n","train final score: 4438/9500 = 46.72% correct\n","GPT claims that 6 + 7 = 14 but gt is 13\n","GPT claims that 7 + 6 = 14 but gt is 13\n","GPT claims that 5 + 8 = 14 but gt is 13\n","GPT claims that 7 + 6 = 14 but gt is 13\n","GPT claims that 4 + 7 = 12 but gt is 11\n","test final score: 478/500 = 95.60% correct\n","saving model with new top score of 4916.0\n","iter_dt 1289.37ms; iter 510: train loss 0.78817\n","iter_dt 1285.34ms; iter 520: train loss 0.68383\n","iter_dt 1285.09ms; iter 530: train loss 0.77138\n","iter_dt 1289.16ms; iter 540: train loss 0.79015\n","iter_dt 1286.23ms; iter 550: train loss 0.80326\n","iter_dt 1282.58ms; iter 560: train loss 0.80372\n","iter_dt 1287.62ms; iter 570: train loss 0.76895\n","iter_dt 1282.97ms; iter 580: train loss 0.77502\n","iter_dt 1280.33ms; iter 590: train loss 0.80437\n","iter_dt 1284.25ms; iter 600: train loss 0.66776\n","iter_dt 1281.86ms; iter 610: train loss 0.77791\n","iter_dt 1284.74ms; iter 620: train loss 0.71506\n","iter_dt 1282.73ms; iter 630: train loss 0.81680\n","iter_dt 1280.99ms; iter 640: train loss 0.73503\n","iter_dt 1284.05ms; iter 650: train loss 0.70922\n","iter_dt 1283.72ms; iter 660: train loss 0.68983\n","iter_dt 1284.93ms; iter 670: train loss 0.70163\n","iter_dt 1285.16ms; iter 680: train loss 0.69951\n","iter_dt 1283.44ms; iter 690: train loss 0.71010\n","iter_dt 1281.49ms; iter 700: train loss 0.64047\n","iter_dt 1284.26ms; iter 710: train loss 0.72768\n","iter_dt 1282.34ms; iter 720: train loss 0.77517\n","iter_dt 1281.99ms; iter 730: train loss 0.71889\n","iter_dt 1281.08ms; iter 740: train loss 0.60875\n","iter_dt 1278.51ms; iter 750: train loss 0.66079\n","iter_dt 1282.59ms; iter 760: train loss 0.60613\n","iter_dt 1285.04ms; iter 770: train loss 0.71013\n","iter_dt 1282.83ms; iter 780: train loss 0.70811\n","iter_dt 1286.95ms; iter 790: train loss 0.72595\n","iter_dt 1282.27ms; iter 800: train loss 0.69260\n","iter_dt 1280.35ms; iter 810: train loss 0.72117\n","iter_dt 1284.65ms; iter 820: train loss 0.66768\n","iter_dt 1284.27ms; iter 830: train loss 0.71560\n","iter_dt 1284.80ms; iter 840: train loss 0.65704\n","iter_dt 1281.95ms; iter 850: train loss 0.63049\n","iter_dt 1284.81ms; iter 860: train loss 0.66287\n","iter_dt 1281.44ms; iter 870: train loss 0.72940\n","iter_dt 1286.07ms; iter 880: train loss 0.68275\n","iter_dt 1286.68ms; iter 890: train loss 0.53746\n","iter_dt 1291.22ms; iter 900: train loss 0.71845\n","iter_dt 1285.60ms; iter 910: train loss 0.72006\n","iter_dt 1286.40ms; iter 920: train loss 0.71335\n","iter_dt 1284.39ms; iter 930: train loss 0.65952\n","iter_dt 1289.67ms; iter 940: train loss 0.66746\n","iter_dt 1283.45ms; iter 950: train loss 0.74016\n","iter_dt 1282.17ms; iter 960: train loss 0.71864\n","iter_dt 1283.63ms; iter 970: train loss 0.64102\n","iter_dt 1284.29ms; iter 980: train loss 0.67894\n","iter_dt 1283.42ms; iter 990: train loss 0.65627\n","iter_dt 1285.69ms; iter 1000: train loss 0.61507\n","GPT claims that 407 + 198 = 705 but gt is 605\n","GPT claims that 95 + 99 = 94 but gt is 194\n","GPT claims that 62 + 260 = 312 but gt is 322\n","GPT claims that 69 + 329 = 388 but gt is 398\n","GPT claims that 451 + 61 = 502 but gt is 512\n","train final score: 4965/9500 = 52.26% correct\n","test final score: 500/500 = 100.00% correct\n","saving model with new top score of 5465.0\n","iter_dt 1285.15ms; iter 1010: train loss 0.65299\n","iter_dt 1287.90ms; iter 1020: train loss 0.61054\n","iter_dt 1288.15ms; iter 1030: train loss 0.65300\n","iter_dt 1288.53ms; iter 1040: train loss 0.64905\n","iter_dt 1285.84ms; iter 1050: train loss 0.70326\n","iter_dt 1288.02ms; iter 1060: train loss 0.65131\n","iter_dt 1283.97ms; iter 1070: train loss 0.70760\n","iter_dt 1284.12ms; iter 1080: train loss 0.64245\n","iter_dt 1287.35ms; iter 1090: train loss 0.63081\n","iter_dt 1283.68ms; iter 1100: train loss 0.69222\n","iter_dt 1284.14ms; iter 1110: train loss 0.64378\n","iter_dt 1287.22ms; iter 1120: train loss 0.68254\n","iter_dt 1285.00ms; iter 1130: train loss 0.69114\n","iter_dt 1281.22ms; iter 1140: train loss 0.59984\n","iter_dt 1283.39ms; iter 1150: train loss 0.60075\n","iter_dt 1287.89ms; iter 1160: train loss 0.57024\n","iter_dt 1288.30ms; iter 1170: train loss 0.56328\n","iter_dt 1286.67ms; iter 1180: train loss 0.53499\n","iter_dt 1280.10ms; iter 1190: train loss 0.47578\n","iter_dt 1282.58ms; iter 1200: train loss 0.57719\n","iter_dt 1288.41ms; iter 1210: train loss 0.54006\n","iter_dt 1288.66ms; iter 1220: train loss 0.57031\n","iter_dt 1285.59ms; iter 1230: train loss 0.57100\n","iter_dt 1288.48ms; iter 1240: train loss 0.62418\n","iter_dt 1285.40ms; iter 1250: train loss 0.58764\n","iter_dt 1287.96ms; iter 1260: train loss 0.51379\n","iter_dt 1286.88ms; iter 1270: train loss 0.54237\n","iter_dt 1286.14ms; iter 1280: train loss 0.59820\n","iter_dt 1284.72ms; iter 1290: train loss 0.42810\n","iter_dt 1284.09ms; iter 1300: train loss 0.52404\n","iter_dt 1285.70ms; iter 1310: train loss 0.41395\n","iter_dt 1285.28ms; iter 1320: train loss 0.43784\n","iter_dt 1284.65ms; iter 1330: train loss 0.44733\n","iter_dt 1282.34ms; iter 1340: train loss 0.42864\n","iter_dt 1282.96ms; iter 1350: train loss 0.39209\n","iter_dt 1287.82ms; iter 1360: train loss 0.35102\n","iter_dt 1282.68ms; iter 1370: train loss 0.35643\n","iter_dt 1285.26ms; iter 1380: train loss 0.34987\n","iter_dt 1287.39ms; iter 1390: train loss 0.36410\n","iter_dt 1282.78ms; iter 1400: train loss 0.35941\n","iter_dt 1285.37ms; iter 1410: train loss 0.33570\n","iter_dt 1288.83ms; iter 1420: train loss 0.27716\n","iter_dt 1287.82ms; iter 1430: train loss 0.29488\n","iter_dt 1287.55ms; iter 1440: train loss 0.28816\n","iter_dt 1286.79ms; iter 1450: train loss 0.29214\n","iter_dt 1288.02ms; iter 1460: train loss 0.22741\n","iter_dt 1284.98ms; iter 1470: train loss 0.22208\n","iter_dt 1287.59ms; iter 1480: train loss 0.22777\n","iter_dt 1289.43ms; iter 1490: train loss 0.22642\n","iter_dt 1285.84ms; iter 1500: train loss 0.24039\n","GPT claims that 430 + 330 = 750 but gt is 760\n","GPT claims that 697 + 96 = 693 but gt is 793\n","GPT claims that 508 + 542 = 2050 but gt is 1050\n","GPT claims that 999 + 295 = 1194 but gt is 1294\n","GPT claims that 5973133467 + 6399753985 = 12172887452 but gt is 12372887452\n","train final score: 5608/9500 = 59.03% correct\n","test final score: 500/500 = 100.00% correct\n","saving model with new top score of 6108.0\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-be2d6efafcae>\u001b[0m in \u001b[0;36m<cell line: 67>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;31m# run the optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/content/minGPT/mingpt/trainer.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# backprop and update the parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_to_none\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_norm_clip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["def convert_to_list(number, context_length):\n","    digits = []\n","    while number > 0:\n","        digits.insert(0, number % 10)\n","        number //= 10\n","    if len(digits) < context_length:\n","        digits = [0] * (context_length - len(digits)) + digits\n","    return digits"],"metadata":{"id":"x4p-H4IFTrIx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def render_equation(a, b, c):\n","    a_str = \"\".join(str(d) for d in a).lstrip('0') or '0'\n","    b_str = \"\".join(str(d) for d in b).lstrip('0') or '0'\n","    c_str = \"\".join(str(d) for d in c).lstrip('0') or '0'\n","    equation_str = \"{} + {} = {}\".format(a_str, b_str, c_str)\n","    return equation_str"],"metadata":{"id":"tjcfvvPLVdsm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def reverse_last_digits(arr, ndigits):\n","    last_digits = arr[-ndigits - 1:]\n","    last_digits.reverse()\n","    return last_digits"],"metadata":{"id":"BLJ_MgliUuh5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["a = convert_to_list(999, train_dataset.config.ndigit)\n","b = convert_to_list(1, train_dataset.config.ndigit)"],"metadata":{"id":"g5sRDLEtTr-Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = torch.tensor([a + b], dtype=torch.long).to(trainer.device)\n","result = model.generate(x, train_dataset.config.ndigit+1, do_sample=False)"],"metadata":{"id":"3AgdlaZJN8NR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["c = reverse_last_digits(list(result[0].cpu().numpy()), train_dataset.config.ndigit)"],"metadata":{"id":"87iRGMbzUM_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["render_equation(a, b, c)"],"metadata":{"id":"0cGYYw5wVeXh"},"execution_count":null,"outputs":[]}]}